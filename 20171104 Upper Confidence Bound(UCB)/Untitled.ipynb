{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化學習(Reinforcement Learning)是機器學習的一個分支，也稱為線上學習、評等學習。 它用於解決交互問題，在時間&ensp;$t$&ensp;觀察到的數據被認為決定在時間&ensp;$t + 1$&ensp;採取的行動。它也被用於人工智能，例如最近的Alpha Go，就是透過反覆試驗學習後更新其行為準則，而作者是在章節中使用機器狗的例子。<br> \n",
    "而課程中有介紹其中兩種：<br>\n",
    "+ Upper Confidence Bound (UCB)。<br>\n",
    "+ Thompson Sampling。<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit Problem(多臂吃角子老虎問題)<Hr>\n",
    "  [吃角子老虎機問題補充1](https://blog.yoctol.com/%E8%B3%AD%E5%BE%92%E7%9A%84%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A71-%E5%90%83%E8%A7%92%E5%AD%90%E8%80%81%E8%99%8E-bandit-%E5%95%8F%E9%A1%8C-62da60b58e3e)<br>\n",
    "  [吃角子老虎機問題補充2](https://zhuanlan.zhihu.com/p/21388070)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">情境說明</span>：有一天，當有一個賭客進入了一間賭場，想要透過吃角子老虎機以小博大，但發現在賭場中居然有一排一模一樣並且擺在一起的吃角子老虎機，他不知道應該選哪一台吃角子老虎機好，因为賭客曾聽人過有些機器中獎機率比較高，有些中獎機率比較低。那麼請問，假如賭客今天有1000塊錢，玩一次需要支付1塊錢，應該怎麼玩可以讓賭客獲得最大的收益?<br>\n",
    "### <span style=\"color:BLUE\">概念</span>：\n",
    "&ensp;&ensp;因為同時有很多台吃角子老虎機供選擇，每一台機器可以得到的期望報酬&ensp;(Expected Reward，指的是玩吃角子老虎機非常多次後得到的平均報酬)&ensp;皆不一樣。站在賭客的立場，目標應該是透過對機器的選擇，從遊戲中獲得最大「期望報酬」。<br>\n",
    "&ensp;&ensp;而剛開始賭客沒有任何各機器的期望報酬資訊，因此賭徒需要<span style=\"color:red\">探勘</span> &ensp;(exploration) &ensp;各個機台報酬的可能性，也就是先試玩一段時間。在探勘累積了足夠次數，對於每台機器的期望報酬有了一定的了解之後，賭徒就可以開始進行<span style=\"color:red\">開發</span> &ensp; (exploitation) &ensp;，不斷去玩最有潛力（期望報酬最高）的機台，獲取最大的累積報酬。<br>\n",
    "(ps：被稱為<span style=\"color:BLUE\">多臂吃角子老虎機</span> (multi-armed bandit)的原因，因為吃角子老虎機的遊戲方式是拉桿，所以會將吃角子老虎機稱為臂(Arm)，指的則是很多台吃角子老虎機給玩家選擇。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/UCB1_1.PNG)\n",
    "在這裡會假設報酬或是獎勵的產生方式是&ensp;<span style=\"color:red\">隨機式</span>(stochastic bandit)&ensp;: $臂_i$的報酬服從某種固定的機率分配$D_i$，並假設每台機器機率分配不同以及事先不知道每個機器的機率分配。\n",
    "\n",
    "![](images/UCB1.PNG)\n",
    "在此例的五台機器中，對我們最有利的是最後一台，因為他的報酬分布較高，這台機器是左偏分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課程中提到的[Paper](Using Confidence Bounds for Exploitation-Exploration Trade-offs.pdf)有放在資料夾中，這個問題最常被用在廣告上，當有不同的活動圖片時，哪一種可以吸引較多的點擊。最簡單就是ABtest，但當文案太多的時候會需要很多的樣本去證明。所以ABtest只是單純地勘查當然吃角子老虎機的問題也可以應用在廣告文案的選擇或是"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper Confidence Bound (UCB)<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
